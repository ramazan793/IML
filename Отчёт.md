# Наблюдения по начальной инициализации
0. Случайная иницилизация довольно-таки плохо справляется с "шариками", например это видно в *интересном тесте 1*(который находится в самом конце блокнота)
1. Заметил, что при эвристике `max_dist`, в отличие от случайной инициализации, площади кластеров становятся равны(+-) на равномерно распределенном датасете.
2. `max_dist` часто неправильно кластеризует, особенно это видно на *интересном тесте 1*. Связано это с тем, что при выборе больше, чем 4х центров, 5ые и далее центры начинают выбираться в нелогичных с точки зрения кластеризации местах(это видно на дефолтном тесте равн. распределения, в данном примере 5ые и далее совпадают с уже выбранными центрами). Необходима модификация.
3. Модификация: начиная с `mod_start`-го центра выбирать не самую удаленную, а РАВНОУДАЛЕННУЮ точку.
4. Реализовал `max_dist_mod`(3 пункт), используя в качестве характеристика равноудаленности - ковариацию расстояний до центров. Действительно, при правильном выборе `mod_start`, можно получить хорошие результаты. В случае *интересного теста 1* (5 шариков) необходимо выбрать `mod_start` = 4 (т. е. начиная с 4го центра, выбор будет идти по хар-ке равноудаленности от остальных центров). Вообще, кажется идеальным для двумерного случая брать `mod_start` = 4, для n-мерного = 2^n.

# Для каких задач подходит k_means? 
1. k_means хорошо справляется с кластеризацией равномерно распределенных данных
2. k_means, на мой взгляд, зависит от случайности, а именно от того, как лягут начальные центроиды(даже если случайно выбирается только одна). Хотя, полагаю, что можно и от зависимости от рандома избавиться, если выбирать первую центроиду по какой-нибудь характеристике, например плотности.
3. Из второго следует, что с **первого раза** **постоянно** получать идеальную кластеризацию для произвольного датасета невозможно, придется, ориентируясь на различные метрики подгонять.
4. k_means подходит для двумерных задач с шариками. Кроме того, что k_means может их кластеризовать правильно с первого раза, так даже если и не с первого, легко будет подогнать, так как мы визуально видим кластеризацию.
5. k_means плох для остальных типов задач, например он не сможет кластеризовать концентрические окружности или луны 

# Скорость
N = кол-во элементов в выборке <br />
k = кол-во кластеров <br />
1. **k = 2**
    1. Как видно из графиков, при **N <= 10000**, все алгоритмы отрабатывают по времени +- одинаково, кроме k_means++, график которой в некоторых случаях находится чуть ниже остальных.
    2. При **N <= 100000** ситуация аналогичная, k_means++ опережает остальные алгоритмы на ~7 единиц времени(секунд, полагаю).
2. **k = 6**
    1. При **N <= 10000**, k_means++ почти в 2 раза опережает по времени остальные алгоритмы.
    2. при **N <= 100000**, k_means++ тоже лидирует, но разница не особо большая
3. **k = 100**
    1. При **N <= 10000**, все алгоритмы показывают себя +- одинаково по времени
    2. При **N <= 100000**, k_means++ опережает остальные алгоритмы чуть ли не в 2 раза.
    
**Вывод**: <br />
В целом зависимость от N - **линейная**. <br />
Можно отметить, что k_means++ в большинстве случаев окажется быстрее остальных алгоритмов. <br />
А при условии что **k >> 1** и **N >> 1** так и вовсе опережает остальные алгоритмы в 2 раза, как показал тест 3B. 

# Выбор числа кластеров
1. Выбирать число кластеров можно просто применяя алгоритм k_means начиная с 2х до N кластеров.
2. Получив результаты по каждому i от 2 до N, выбрать то i, у которого наилучшая матрица попарных расстояний(либо наибольший score) 

# Что ещё можно доделать?
1. Учесть кол-во параметров > 2. 
2. Автоматизировать выбор числа кластеров(наверное, по score).